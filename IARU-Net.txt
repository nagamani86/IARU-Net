class Proposed_architecture:
    def __init__(self,nfilters):
        self.f_1,self.f_2,self.f_3,self.f_4 = nfilters
        self.inputs = Input(shape = (224,224,3))
        
        
    def inception_block_dim_reduce(self,input_layer, filters, activation='relu'):
        filter1x1, filter3x3, filter5x5, reduce3x3, reduce5x5, pool_proj = filters

        conv1x1=BatchNormalization(axis=3)(input_layer)
        conv1x1 = Conv2D(filter1x1, kernel_size=(1,1), padding='same', activation=activation)(conv1x1)


        conv3x3_reduce = Conv2D(reduce3x3, kernel_size=(1,1), padding='same', activation=activation)(input_layer)
        conv3x3 = Conv2D(filter3x3, kernel_size=(3,3), padding='same',activation=activation)(conv3x3_reduce)

        conv5x5_reduce = Conv2D(reduce5x5, kernel_size=(1,1), padding='same', activation=activation)(input_layer)
        conv5x5 = Conv2D(filter5x5, kernel_size=(5,5), padding='same',activation=activation)(conv5x5_reduce)

        pooling = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_layer)
        pooling = BatchNormalization(axis=3)(pooling)
        pool_proj = Conv2D(pool_proj, kernel_size=(1,1), padding='same', activation=activation)(pooling)
        output_layer = concatenate([conv1x1, conv3x3, conv5x5, pool_proj])
        return output_layer
    
    def conv_max(self,x,fil):
        pool = MaxPooling2D((2,2),strides = (2,2))(x)
        conv = Conv2D(fil*2,(3,3),padding="same")(pool)
        return conv
    
    def encoder(self):
        print(self.f_1,self.f_2,self.f_3,self.f_4)
        o1 = self.inception_block_dim_reduce(self.inputs, [self.f_1//4]*6, activation='relu')# (224, 224, 32)
        pool1 = MaxPooling2D((2,2),strides = (2,2))(o1)
        pool2 = MaxPooling2D((2,2),strides = (2,2))(o1)
        pool = Add()([pool1,pool2])
        conv = Conv2D(self.f_2,(3,3),padding="same")(pool)
        conv = BatchNormalization(axis = 3)(conv)

        o2 = self.inception_block_dim_reduce(conv, [self.f_2//4]*6, activation='relu')#(112, 112, 64)
        conv = self.conv_max(Add()([conv,o2]),self.f_2)
        conv = BatchNormalization(axis = 3)(conv)
        
        o3 = self.inception_block_dim_reduce(conv, [self.f_3//4]*6, activation='relu')#(56, 56, 128)
        conv = Conv2D(self.f_3,(3,3),padding="same")(conv)
        conv = self.conv_max(Add()([conv,o3]),self.f_3)
        conv = BatchNormalization(axis = 3)(conv)

        o4 = self.inception_block_dim_reduce(conv, [self.f_4//4]*6, activation='relu')#(28, 28, 256)
        conv = self.conv_max(Add()([conv,o4]),self.f_4)
        conv = BatchNormalization(axis = 3)(conv)
        low_layer = self.inception_block_dim_reduce(conv, [1024]*6, activation='relu')
        
        return o1,o2,o3,o4,low_layer
        
        
    def attention(self,input1,input2):
        filters = input1.shape[-1]*2
        conv1 = Conv2D(filters,(3,3),padding='same',activation = 'relu')(input1)

        conv2 = Conv2DTranspose(filters,(2,2),strides = (2,2),padding='same',activation = 'relu')(input2)
        conv2 = MaxPooling2D((2,2),strides = (1,1),padding = "same")(conv2)
        conv2 = BatchNormalization(axis = 3)(conv2)
        sig  = Activation('sigmoid')(conv2)
        a_out = multiply([conv1,sig])

        return a_out,conv2
    def add_conv(self,f,layer1,layer2):
        add = Add()([layer1,layer2])
        conv = Conv2D(f,(3,3),padding='same',activation = 'relu')(add)
        return conv

    def upsamp(self,iterations,layer):
        for i in range(iterations):
            layer = Conv2DTranspose(self.f_1,(2,2),strides = (2,2),padding='valid',activation = 'relu')(layer)
        return layer
    
    def UnInc_att_res(self):
        o1,o2,o3,o4,low_layer = self.encoder()
        
        a1,up1 = self.attention(o4,low_layer)         #(None, 28, 28, 512)
        add = self.add_conv(self.f_4,a1,up1)
        
        a2,up2 = self.attention(o3,add)               #(None, 56, 56, 256)
        add = self.add_conv(self.f_3,a2,up2)
        
        a3,up3 = self.attention(o2,add)               #None, 112, 112, 128)
        add = self.add_conv(self.f_2,a3,up3)
        
        a4,up4 = self.attention(o1,add)               #(None, 224, 224, 64)
        add = self.add_conv(self.f_1,a4,up4)
        
        a1_o = self.upsamp(3,a1)                 #( 224, 224, 32)
        a2_o = self.upsamp(2,a2)                 #( 224, 224, 32)
        a3_o = self.upsamp(1,a3)                 #( 224, 224, 32)
        a4_o = Conv2D(self.f_1,(1,1),padding = 'same')(a4)# (224, 224, 32)

        final = Add()([a1_o,a2_o,a3_o,a4_o,add])  
        output = Conv2D(1,(1,1),activation = 'sigmoid')(final)
        model = Model(self.inputs,output)
        
        #learning_rate = 0.001
        #optimizer = Adam(learning_rate=learning_rate)
        #model.compile(optimizer=optimizer, loss=DiceCoefficientLoss(), metrics=['accuracy'])

        return model
obj = Proposed_architecture([32,64,128, 256])
model = obj.UnInc_att_res()